{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API for reinforcement learning - Basic Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original source: https://www.gymlibrary.dev/content/basic_usage/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interacting with the Environment\n",
    "Gym implements the classic “agent-environment loop”\n",
    "\n",
    "The **agent** performs some actions in the **environment** (usually by passing some control inputs to the environment, e.g. torque inputs of motors) and **observes** how the environment’s **state** changes. One such action-observation exchange is referred to as a **timestep**.\n",
    "\n",
    "The goal in RL is to **manipulate the environment** in some specific way. For instance, we want the agent to navigate a robot to a specific point in space. If it succeeds in doing this (or makes some progress towards that goal), it will receive a **positive reward** alongside the observation for this timestep. The reward may also be **negative or 0**, if the agent did not yet succeed (or did not make any progress). The agent will then be trained to **maximize** the reward it accumulates over many timesteps.\n",
    "\n",
    "After some timesteps, the environment may enter a **terminal state**. For instance, the robot may have crashed! In that case, we want to **reset the environment** to a new initial state. The environment issues a done signal to the agent if it enters such a terminal state. Not all done signals must be triggered by a “catastrophic failure”: Sometimes we also want to issue a done signal after a fixed number of timesteps, or if the agent has succeeded in completing some task in the environment.\n",
    "\n",
    "Let’s see what the agent-environment loop looks like in Gym. This example will run an instance of LunarLander-v2 environment for 1000 timesteps. Since we pass render_mode=\"human\", you should see a window pop up rendering the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(1000):\n",
    "   #action = policy(observation)  # User-defined policy function\n",
    "   action = env.action_space.sample()\n",
    "   observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "   if terminated or truncated:\n",
    "      observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "for _ in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('neoland')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "13aa3a90a5e641235fea1601ecd5c89f84592ee55de82aab0f64a36e64690c8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
